{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import skimage.feature\n",
      "import numpy as np\n",
      "from os import listdir\n",
      "from multiprocessing import Pool, cpu_count\n",
      "from pylab import imread\n",
      "from time import time\n",
      "from sklearn import grid_search, metrics\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "import pickle"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 301
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#from sklearn import datasets\n",
      "\n",
      "# import some data to play with\n",
      "#test_data = datasets.load_iris()\n",
      "#X = test_data.data\n",
      "#Y = test_data.target\n",
      "\n",
      "test_data = pickle.load( open( \"extracted_features.p\", \"rb\" ) )\n",
      "X = test_data[0]\n",
      "Y = test_data[1]\n",
      "\n",
      "# take the first 100 as training\n",
      "train_frac = 0.8\n",
      "train = int(len(Y))*train_frac\n",
      "Xtr = X[:train]\n",
      "Ytr = Y[:train]\n",
      "print(\"training size: \" + str(len(Ytr)))\n",
      "\n",
      "# testing set\n",
      "test_X = X[train:]\n",
      "test_Y = Y[train:]\n",
      "print(\"testing size: \" + str(len(test_Y)))\n",
      "\n",
      "before_create = time()\n",
      "clf = RandomForestClassifier(n_estimators=50, criterion='gini', max_depth=None, min_samples_split=2, \n",
      "                             min_samples_leaf=1, max_features='auto', bootstrap=True, oob_score=False, \n",
      "                             n_jobs=1, random_state=None, verbose=0, min_density=None, compute_importances=True)\n",
      "before_fit = time()\n",
      "clf.fit(Xtr, Ytr)\n",
      "\n",
      "# now do the prediction\n",
      "before_pred = time()\n",
      "Y_pred = clf.predict(test_X)\n",
      "after_pred = time()\n",
      "\n",
      "print \"Following times: \" + \"\\n creation:\" + \\\n",
      "    str(round(before_create-before_fit, 3)) + \" s, fit: \" + \\\n",
      "    str(round(before_fit-before_pred, 3)) + \" s, predict: \" + \\\n",
      "    str(round(before_pred-after_pred, 3)) + \" s, total: \" + \\\n",
      "    str(round(before_create-after_pred, 3)) + \" s\"\n",
      "\n",
      "# how well did we do?\n",
      "from sklearn import cross_validation\n",
      "from sklearn.cross_validation import cross_val_score\n",
      "\n",
      "def print_cv_score_summary(model, xx, yy, cv):\n",
      "    scores = cross_val_score(model, xx, yy, cv=cv, n_jobs=1)\n",
      "    print(\"mean: {:3f}, stdev: {:3f}\".format(\n",
      "        np.mean(scores), np.std(scores)))\n",
      "\n",
      "print_cv_score_summary(clf,X,Y,cv=cross_validation.KFold(len(Y), 5, shuffle=True))\n",
      "print clf.feature_importances_\n",
      "print X[:5,:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "training size: 96\n",
        "testing size: 24\n",
        "Following times: \n",
        " creation:-0.002 s, fit: -0.079 s, predict: -0.004 s, total: -0.085 s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "mean: 0.625000, stdev: 0.079057"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[ 0.10210447  0.05250456  0.04618657  0.05609351  0.04863564  0.025996\n",
        "  0.04996572  0.05448961  0.04244206  0.02989158  0.05662036  0.04250993\n",
        "  0.05816769  0.03971838  0.09448798  0.03737032  0.03592066  0.07730137\n",
        "  0.04959361]\n",
        "[[  1.83414000e+05   2.03726504e+02   1.80533678e+02   1.64588243e+02\n",
        "    1.82949475e+02   1.93666667e+02   5.53809192e+01   2.18000000e+02\n",
        "    1.91000000e+02   1.73000000e+02   5.27742156e+01   5.64404489e+01\n",
        "    5.79835680e+01   4.30338469e+00   1.00000000e+00   1.33117645e+01\n",
        "    6.75430992e+00   8.33333333e-01   2.11033898e+01]\n",
        " [  1.73604000e+05   1.85957680e+02   1.89446793e+02   1.76820142e+02\n",
        "    1.84074872e+02   2.09666667e+02   6.89327056e+01   1.97000000e+02\n",
        "    2.19000000e+02   2.27000000e+02   6.05768236e+01   7.08863345e+01\n",
        "    8.27167153e+01   7.36082118e+00   1.00000000e+00   1.65519538e+01\n",
        "    1.23650578e+01   2.00000000e+00   2.32144481e+01]\n",
        " [  4.26600000e+05   8.43603165e+01   1.21159747e+02   1.35140056e+02\n",
        "    1.13553373e+02   1.31333333e+02   5.48818138e+01   7.70000000e+01\n",
        "    1.37000000e+02   1.33000000e+02   5.38745115e+01   5.89403394e+01\n",
        "    6.87092549e+01   1.00864252e+01   2.00000000e+00   2.28534184e+01\n",
        "    1.78542370e+01   3.75000000e+00   3.45121542e+01]\n",
        " [  3.62250000e+05   1.07507321e+02   1.11441300e+02   9.79021615e+01\n",
        "    1.05616928e+02   1.08333333e+02   7.06375897e+01   1.11000000e+02\n",
        "    1.22000000e+02   8.70000000e+01   7.45188171e+01   7.61737860e+01\n",
        "    6.40883809e+01   1.35927274e+01   6.16666667e+00   1.89414812e+01\n",
        "    1.10543299e+01   5.16666667e+00   1.59610283e+01]\n",
        " [  1.17964800e+06   1.51252314e+02   1.57446948e+02   1.55376996e+02\n",
        "    1.54692086e+02   1.69333333e+02   4.46977074e+01   1.65000000e+02\n",
        "    1.72000000e+02   1.73000000e+02   4.28821392e+01   4.46821195e+01\n",
        "    4.74526546e+01   3.49084176e+00   1.58333333e+00   1.17708178e+01\n",
        "    4.91392983e+00   2.00000000e+00   1.54652065e+01]]\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "//anaconda/python.app/Contents/lib/python2.7/site-packages/sklearn/ensemble/forest.py:783: DeprecationWarning: Setting compute_importances is no longer required as version 0.14. Variable importances are now computed on the fly when accessing the feature_importances_ attribute. This parameter will be removed in 0.16.\n",
        "  DeprecationWarning)\n"
       ]
      }
     ],
     "prompt_number": 312
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# load data\n",
      "test_data = pickle.load( open( \"extracted_features.p\", \"rb\" ) )\n",
      "X = test_data[0]\n",
      "Y = test_data[1]\n",
      "\n",
      "# explore 3 different forest sizes and 3 choices of mtry\n",
      "parameters = {'n_estimators':[20,50,100],  'max_features':[2,5,10,15], 'compute_importances':[True]}\n",
      "rf_opt = grid_search.GridSearchCV(RandomForestClassifier(), parameters,\\\n",
      "                                   score_func=metrics.accuracy_score, n_jobs = 1, cv = 5)\n",
      "#rf_opt = rf_tune.fit(Xtr, Ytr)\n",
      "rf_opt.fit(X, Y)\n",
      "print(\"Best score: \" + str(rf_opt.best_score_) + \", random guessing: \" + str(1/float(len(np.unique(Y)))) + \"\\n\")\n",
      "print(\"Optimal Model:\\n\" + str(rf_opt.best_estimator_) + \"\\n\")\n",
      "print(\"Best Parameters:\" + str(rf_opt.best_params_) + \"\\n\")\n",
      "\n",
      "clf_best = rf_opt.best_estimator_\n",
      "print(\"Feature importances:\" + str(clf_best.feature_importances_) + \"\\n\")\n",
      "#print(\"Grid Results:\")\n",
      "#rf_opt.grid_scores_\n",
      "\n",
      "feature_names = ['pixel count','avg red','avg green','avg blue','avg lum',\n",
      "                 'median lum','std lum','median red','median green','median blue',\n",
      "                 'std red','std green','std blue','avg lum v-edges','median lum v-edges',\n",
      "                 'std lum v-edges','avg lum h-edges','median lum h-edges','std h-edges']\n",
      "priority_features = [y for (x,y) in sorted(zip(clf_best.feature_importances_, feature_names), reverse=True)]\n",
      "print \"Feature names:\" + str(feature_names) + \"\\n\"\n",
      "print \"Sorted feats:\" + str(priority_features)\n",
      "\n",
      "#clf2 = rf_opt.best_estimator_\n",
      "#clf2.fit(X, Y)\n",
      "#Y_pred = clf2.predict(X)\n",
      "#accuracy_array = (Y_pred - Y) == 0\n",
      "#print 'accuracy percent:', sum(accuracy_array)/float(len(accuracy_array))\n",
      "#print \"built-in:\", metrics.accuracy_score(Y, Y_pred)\n",
      "#print accuracy_array\n",
      "rf_opt.grid_scores_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Best score: 0.254736842105, random guessing: 0.0526315789474\n",
        "\n",
        "Optimal Model:\n",
        "RandomForestClassifier(bootstrap=True, compute_importances=True,\n",
        "            criterion=gini, max_depth=None, max_features=10,\n",
        "            min_density=None, min_samples_leaf=1, min_samples_split=2,\n",
        "            n_estimators=50, n_jobs=1, oob_score=False, random_state=None,\n",
        "            verbose=0)\n",
        "\n",
        "Best Parameters:{'compute_importances': True, 'max_features': 10, 'n_estimators': 50}\n",
        "\n",
        "Feature importances:[ 0.07760447  0.05501518  0.06449657  0.04609492  0.0381496   0.03769451\n",
        "  0.0446802   0.04367708  0.04148278  0.04294697  0.05507764  0.05168777\n",
        "  0.06501132  0.05901948  0.06016259  0.05566134  0.05382035  0.04796519\n",
        "  0.05975205]\n",
        "\n",
        "Feature names:['pixel count', 'avg red', 'avg green', 'avg blue', 'avg lum', 'median lum', 'std lum', 'median red', 'median green', 'median blue', 'std red', 'std green', 'std blue', 'avg lum v-edges', 'median lum v-edges', 'std lum v-edges', 'avg lum h-edges', 'median lum h-edges', 'std h-edges']\n",
        "\n",
        "Sorted feats:['pixel count', 'std blue', 'avg green', 'median lum v-edges', 'std h-edges', 'avg lum v-edges', 'std lum v-edges', 'std red', 'avg red', 'avg lum h-edges', 'std green', 'median lum h-edges', 'avg blue', 'std lum', 'median red', 'median blue', 'median green', 'avg lum', 'median lum']\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "//anaconda/python.app/Contents/lib/python2.7/site-packages/sklearn/grid_search.py:466: DeprecationWarning: Passing function as ``score_func`` is deprecated and will be removed in 0.15. Either use strings or score objects.The relevant new parameter is called ''scoring''.\n",
        "  self.loss_func, self.score_func, self.scoring)\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 323,
       "text": [
        "[mean: 0.21684, std: 0.02890, params: {'compute_importances': True, 'max_features': 2, 'n_estimators': 20},\n",
        " mean: 0.22316, std: 0.02613, params: {'compute_importances': True, 'max_features': 2, 'n_estimators': 50},\n",
        " mean: 0.23053, std: 0.01348, params: {'compute_importances': True, 'max_features': 2, 'n_estimators': 100},\n",
        " mean: 0.21158, std: 0.01124, params: {'compute_importances': True, 'max_features': 5, 'n_estimators': 20},\n",
        " mean: 0.23579, std: 0.03637, params: {'compute_importances': True, 'max_features': 5, 'n_estimators': 50},\n",
        " mean: 0.23579, std: 0.01952, params: {'compute_importances': True, 'max_features': 5, 'n_estimators': 100},\n",
        " mean: 0.22947, std: 0.02068, params: {'compute_importances': True, 'max_features': 10, 'n_estimators': 20},\n",
        " mean: 0.25474, std: 0.01134, params: {'compute_importances': True, 'max_features': 10, 'n_estimators': 50},\n",
        " mean: 0.24632, std: 0.01021, params: {'compute_importances': True, 'max_features': 10, 'n_estimators': 100},\n",
        " mean: 0.22737, std: 0.01219, params: {'compute_importances': True, 'max_features': 15, 'n_estimators': 20},\n",
        " mean: 0.22737, std: 0.01021, params: {'compute_importances': True, 'max_features': 15, 'n_estimators': 50},\n",
        " mean: 0.24316, std: 0.01742, params: {'compute_importances': True, 'max_features': 15, 'n_estimators': 100}]"
       ]
      }
     ],
     "prompt_number": 323
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Helper code from: Christopher Klein, Joshua Bloom\n",
      "\"\"\"\n",
      "from os import listdir\n",
      "from multiprocessing import Pool, cpu_count\n",
      "from pylab import imread\n",
      "from time import time\n",
      "\n",
      "MYDIRECTORY = \"/Users/sean/Desktop/50_categories\"\n",
      "\n",
      "def split_seq(seq, size):\n",
      "    \"\"\" Splits list \"seq\" into a list of \"size\" number of smaller, roughly equal length, lists \"\"\"\n",
      "    newseq = []\n",
      "    splitsize = 1.0/size*len(seq)\n",
      "    for i in range(size):\n",
      "        newseq.append(seq[int(round(i*splitsize)):\n",
      "            int(round((i+1)*splitsize))])\n",
      "    return newseq\n",
      "# Our simple feature extraction function. It takes in a list of image paths, \n",
      "# does some measurement on each image, then returns a list of the image paths\n",
      "# paired with the results of the feature measurement.\n",
      "def extract_features(image_path_list):\n",
      "    feature_list = []\n",
      "    for image_path in image_path_list:\n",
      "        image_array = imread(image_path)\n",
      "        feature = feature_1(image_array)\n",
      "        # This feature is simple. You can modify this\n",
      "        # code to produce more complicated features and to produce multiple\n",
      "        # features in one function call.\n",
      "        feature_list.append([image_path, feature])\n",
      "    return feature_list\n",
      "\n",
      "# Features list\n",
      "def feature_1(image_array):\n",
      "    return image_array.size"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Main program starts here ###################################################\n",
      "# We first collect all the local paths to all the images in one list\n",
      "image_paths = []\n",
      "categories = listdir(MYDIRECTORY)\n",
      "for category in categories:\n",
      "    image_names = listdir(MYDIRECTORY  + \"/\" + category)\n",
      "    for name in image_names:\n",
      "        image_paths.append(MYDIRECTORY + \"/\" + category + \"/\" + name)\n",
      "\n",
      "print (\"There should be 4244 images, actual number is \" + \n",
      "    str(len(image_paths)) + \".\")\n",
      "\n",
      "# Then, we run the feature extraction function using multiprocessing.Pool so \n",
      "# so that we can parallelize the process and run it much faster.\n",
      "numprocessors = cpu_count() # To see results of parallelizing, set numprocessors\n",
      "                            # to less than cpu_count().\n",
      "# numprocessors = 1\n",
      "\n",
      "# We have to cut up the image_paths list into the number of processes we want to\n",
      "# run. \n",
      "split_image_paths = split_seq(image_paths, numprocessors)\n",
      "\n",
      "# Ok, this block is where the parallel code runs. We time it so we can get a \n",
      "# feel for the speed up.\n",
      "start_time = time()\n",
      "p = Pool(numprocessors)\n",
      "result = p.map_async(extract_features, split_image_paths)\n",
      "poolresult = result.get()\n",
      "end_time = time()\n",
      "\n",
      "# All done, print timing results.\n",
      "print (\"Finished extracting features. Total time: \" + \n",
      "    str(round(end_time-start_time, 3)) + \" s, or \" + \n",
      "    str( round( (end_time-start_time)/len(image_paths), 5 ) ) + \" s/image.\")\n",
      "# This took about 10-11 seconds on my 2.2 GHz, Core i7 MacBook Pro. It may also\n",
      "# be affected by hard disk read speeds.\n",
      "\n",
      "# To tidy-up a bit, we loop through the poolresult to create a final list of\n",
      "# the feature extraction results for all images.\n",
      "combined_result = []\n",
      "for single_proc_result in poolresult:\n",
      "    for single_image_result in single_proc_result:\n",
      "        combined_result.append(single_image_result)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# try to run through all pics and extract features, except: file being run by grader, not me, so directory does not exist, proceed."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#%load hw_4-machine-learning-parallel-strawman.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 262
    }
   ],
   "metadata": {}
  }
 ]
}