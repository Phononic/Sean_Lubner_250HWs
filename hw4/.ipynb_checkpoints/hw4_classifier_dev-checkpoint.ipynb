{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pickle\n",
      "from os import listdir\n",
      "from time import time\n",
      "from sklearn import grid_search, metrics\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "# load data\n",
      "im_data = pickle.load( open( \"extracted_features.p\", \"rb\" ) )\n",
      "X = im_data[0] # extracted features\n",
      "Y = im_data[1] # target\n",
      "num_classes = len(np.unique(Y)) # number of classes\n",
      "nc = X.shape[1]\n",
      "\n",
      "# optimize number of features\n",
      "parameters = {'max_features':map(lambda x: int(x), [np.ceil(np.sqrt(nc)), np.ceil(nc/3.0), np.ceil(nc/1.5)]),\n",
      "              'n_estimators':[50], 'compute_importances':[True], 'n_jobs':[1]}\n",
      "rf_opt = grid_search.GridSearchCV(RandomForestClassifier(), parameters,\\\n",
      "                                   score_func=metrics.accuracy_score, n_jobs = 1, cv = 4)\n",
      "before_GS = time()\n",
      "rf_opt.fit(X, Y)\n",
      "after_GS = time()\n",
      "best_rf = rf_opt.best_estimator_\n",
      "\n",
      "# ------------------- Output -------------------\n",
      "class feature_priority(object):\n",
      "    def __init__(self, priorities, dec=4):\n",
      "        self.feature_names = ['pixel count','avg red','avg green','avg blue','avg lum',\n",
      "                              'median lum','std lum','median red','median green','median blue',\n",
      "                              'std red','std green','std blue','avg lum v-edges','median lum v-edges',\n",
      "                              'std lum v-edges','avg lum h-edges','median lum h-edges','std h-edges',\n",
      "                              '>thresh h-edges','>thresh v-edges','aspect ratio','image peaks']\n",
      "        self.feature_priorities = map(lambda x: np.round(x, decimals=dec), priorities)\n",
      "        self.sorted_feats = sorted(zip(self.feature_names,self.feature_priorities), key = lambda x: x[1], reverse=True)\n",
      "    \n",
      "    def __str__(self):\n",
      "        outstr = \"Features (most to least important): \\n\"\n",
      "        for (i,j) in self.sorted_feats:\n",
      "            outstr += \"\\t\" + i + \" \"*(25-len(i)) + \"relative importance: \" + str(j) + \"\\n\"\n",
      "        return outstr\n",
      "\n",
      "class prediction_output(object):\n",
      "    def __init__(self, classifier, X, Y):\n",
      "        self.Y_pred = classifier.predict(X)\n",
      "        \n",
      "print(\"Time to run grid search: {0:.3f} sec.  Average of {1:.4f} sec per classifier fit/predict cycle (per parameter combo per CV-fold)\\n\"\\\n",
      "      .format(after_GS-before_GS, (after_GS-before_GS)/(len(rf_opt.grid_scores_)*rf_opt.cv)))\n",
      "print(\"Best score: {0:.2f}% accuracy, vs. random guessing at: {1:.2f}%, for a factor of {2:.1f}X improvement.\" \\\n",
      "      .format(100*rf_opt.best_score_, 100.0/num_classes, rf_opt.best_score_*num_classes))\n",
      "print(\"Best Parameters:\" + str(rf_opt.best_params_) + \"\\n\")\n",
      "\n",
      "feats = feature_priority(best_rf.feature_importances_)\n",
      "print str(feats)\n",
      "print \"Grid Search Scores:\" \n",
      "for score in rf_opt.grid_scores_:\n",
      "    print score"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Time to run grid search: 14.263 sec.  Average of 7.1315 sec per classifier fit/predict cycle (per parameter combo per CV-fold)\n",
        "\n",
        "Best score: 25.71% accuracy, vs. random guessing at: 2.00%, for a factor of 12.9X improvement.\n",
        "Best Parameters:{'compute_importances': True, 'max_features': 13, 'n_jobs': 1, 'n_estimators': 50}\n",
        "\n",
        "Features (most to least important): \n",
        "\tpixel count              relative importance: 0.1267\n",
        "\tavg lum v-edges          relative importance: 0.0665\n",
        "\tstd h-edges              relative importance: 0.0656\n",
        "\tmedian lum v-edges       relative importance: 0.0629\n",
        "\tstd lum v-edges          relative importance: 0.0543\n",
        "\tstd red                  relative importance: 0.0523\n",
        "\tavg lum h-edges          relative importance: 0.0522\n",
        "\tstd blue                 relative importance: 0.0504\n",
        "\tavg blue                 relative importance: 0.0491\n",
        "\tmedian lum h-edges       relative importance: 0.0483\n",
        "\tavg red                  relative importance: 0.0474\n",
        "\tstd green                relative importance: 0.0459\n",
        "\tavg green                relative importance: 0.0455\n",
        "\tmedian blue              relative importance: 0.0427\n",
        "\tmedian green             relative importance: 0.0408\n",
        "\tmedian red               relative importance: 0.04\n",
        "\tstd lum                  relative importance: 0.0392\n",
        "\tavg lum                  relative importance: 0.0359\n",
        "\tmedian lum               relative importance: 0.0345\n",
        "\n",
        "Grid Search Scores:\n",
        "mean: 0.25707, std: 0.00778, params: {'compute_importances': True, 'max_features': 13, 'n_jobs': 1, 'n_estimators': 50}\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "//anaconda/python.app/Contents/lib/python2.7/site-packages/sklearn/grid_search.py:466: DeprecationWarning: Passing function as ``score_func`` is deprecated and will be removed in 0.15. Either use strings or score objects.The relevant new parameter is called ''scoring''.\n",
        "  self.loss_func, self.score_func, self.scoring)\n"
       ]
      }
     ],
     "prompt_number": 456
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#from sklearn import datasets\n",
      "\n",
      "# import some data to play with\n",
      "#test_data = datasets.load_iris()\n",
      "#X = test_data.data\n",
      "#Y = test_data.target\n",
      "\n",
      "test_data = pickle.load( open( \"extracted_features.p\", \"rb\" ) )\n",
      "X = test_data[0]\n",
      "Y = test_data[1]\n",
      "\n",
      "# take the first 100 as training\n",
      "train_frac = 0.8\n",
      "train = int(len(Y))*train_frac\n",
      "Xtr = X[:train]\n",
      "Ytr = Y[:train]\n",
      "print(\"training size: \" + str(len(Ytr)))\n",
      "\n",
      "# testing set\n",
      "test_X = X[train:]\n",
      "test_Y = Y[train:]\n",
      "print(\"testing size: \" + str(len(test_Y)))\n",
      "\n",
      "before_create = time()\n",
      "clf = RandomForestClassifier(n_estimators=50, criterion='gini', max_depth=None, min_samples_split=2, \n",
      "                             min_samples_leaf=1, max_features='auto', bootstrap=True, oob_score=False, \n",
      "                             n_jobs=1, random_state=None, verbose=0, min_density=None, compute_importances=True)\n",
      "before_fit = time()\n",
      "clf.fit(Xtr, Ytr)\n",
      "\n",
      "# now do the prediction\n",
      "before_pred = time()\n",
      "Y_pred = clf.predict(test_X)\n",
      "after_pred = time()\n",
      "\n",
      "print \"Following times: \" + \"\\n creation:\" + \\\n",
      "    str(round(before_create-before_fit, 3)) + \" s, fit: \" + \\\n",
      "    str(round(before_fit-before_pred, 3)) + \" s, predict: \" + \\\n",
      "    str(round(before_pred-after_pred, 3)) + \" s, total: \" + \\\n",
      "    str(round(before_create-after_pred, 3)) + \" s\"\n",
      "\n",
      "# how well did we do?\n",
      "from sklearn import cross_validation\n",
      "from sklearn.cross_validation import cross_val_score\n",
      "\n",
      "def print_cv_score_summary(model, xx, yy, cv):\n",
      "    scores = cross_val_score(model, xx, yy, cv=cv, n_jobs=1)\n",
      "    print(\"mean: {:3f}, stdev: {:3f}\".format(\n",
      "        np.mean(scores), np.std(scores)))\n",
      "\n",
      "print_cv_score_summary(clf,X,Y,cv=cross_validation.KFold(len(Y), 5, shuffle=True))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "training size: 3395\n",
        "testing size: 849\n",
        "Following times: \n",
        " creation:-0.08 s, fit: -2.086 s, predict: -0.063 s, total: -2.229 s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "mean: 0.274973, stdev: 0.017958"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "//anaconda/python.app/Contents/lib/python2.7/site-packages/sklearn/ensemble/forest.py:783: DeprecationWarning: Setting compute_importances is no longer required as version 0.14. Variable importances are now computed on the fly when accessing the feature_importances_ attribute. This parameter will be removed in 0.16.\n",
        "  DeprecationWarning)\n"
       ]
      }
     ],
     "prompt_number": 454
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#%load hw_4-machine-learning-parallel-strawman.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 262
    }
   ],
   "metadata": {}
  }
 ]
}